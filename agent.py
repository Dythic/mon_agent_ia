from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import requests
import time
import torch

class CodeAgent:
    def __init__(self, db_path="./chroma_db", model_name="deepseek-coder:6.7b"):
        print("ü§ñ Initialisation de l'agent...")
        
        # V√©rifier CUDA
        if torch.cuda.is_available():
            print(f"üî• GPU d√©tect√©: {torch.cuda.get_device_name(0)}")
        
        # V√©rifier qu'Ollama fonctionne
        self._wait_for_ollama()
        
        # Charger la base vectorielle
        print("üìö Chargement de la base vectorielle...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}  # CPU pour √©conomiser VRAM
        )
        
        self.vectorstore = Chroma(
            persist_directory=db_path,
            embedding_function=self.embeddings
        )
        
        # V√©rifier que la base contient des donn√©es
        collection = self.vectorstore._collection
        count = collection.count()
        print(f"üìä Base vectorielle charg√©e: {count} documents")
        
        if count == 0:
            raise Exception("‚ùå La base vectorielle est vide ! Lancez d'abord l'indexation.")
        
        # Initialiser le mod√®le Ollama
        print("ü¶ô Connexion √† Ollama...")
        self.llm = OllamaLLM(model=model_name, temperature=0.1)
        
        # Template de prompt sp√©cialis√© pour l'architecture hexagonale
        template = """Tu es un assistant IA expert en architecture hexagonale et Node.js, sp√©cialis√© sur ce projet.

Contexte du projet (architecture hexagonale Node.js):
{context}

Question: {question}

Instructions:
- Utilise le contexte fourni pour donner une r√©ponse pr√©cise et technique
- Si tu parles de code, mentionne les fichiers sp√©cifiques
- Explique les concepts d'architecture hexagonale si n√©cessaire
- Donne des exemples concrets du code du projet

R√©ponse d√©taill√©e:"""
        
        self.prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
        # Cr√©er la cha√Æne RAG
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 5}),
            chain_type_kwargs={"prompt": self.prompt}
        )
        
        print("‚úÖ Agent pr√™t !")
    
    def _wait_for_ollama(self, max_retries=30):
        print("‚è≥ V√©rification d'Ollama...")
        for i in range(max_retries):
            try:
                response = requests.get("http://localhost:11434/api/version", timeout=2)
                if response.status_code == 200:
                    print("‚úÖ Ollama connect√©")
                    return
            except:
                pass
            
            if i == 0:
                print("‚è≥ Attente de la connexion √† Ollama...")
            time.sleep(1)
        
        raise Exception("‚ùå Impossible de se connecter √† Ollama. Lancez 'ollama serve' d'abord.")
    
    def ask(self, question):
        """Poser une question √† l'agent"""
        try:
            response = self.qa_chain.invoke({"query": question})
            return response["result"]
        except Exception as e:
            return f"‚ùå Erreur: {e}"
    
    def search_relevant_files(self, query):
        """Rechercher les fichiers les plus pertinents"""
        docs = self.vectorstore.similarity_search(query, k=5)
        files = []
        for doc in docs:
            source = doc.metadata.get('source', 'Unknown')
            files.append({
                'file': source.split('/')[-1],
                'path': source,
                'content_preview': doc.page_content[:200] + "..."
            })
        return files

if __name__ == "__main__":
    try:
        agent = CodeAgent()
        
        print("\n" + "="*50)
        print("ü§ñ Agent IA - Architecture Hexagonale Node.js")
        print("="*50)
        
        while True:
            question = input("\nüí¨ Votre question (ou 'quit' pour quitter): ")
            if question.lower() in ['quit', 'exit', 'q']:
                break
            
            print("\nüîç Recherche et analyse...")
            answer = agent.ask(question)
            print(f"\nü§ñ R√©ponse:\n{answer}")
            
    except Exception as e:
        print(f"‚ùå Erreur lors de l'initialisation: {e}")