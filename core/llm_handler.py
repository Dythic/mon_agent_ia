"""
Gestion des modÃ¨les de langage
"""
import requests
import time
from typing import Optional
from utils.imports import OllamaLLM, OLLAMA_AVAILABLE

class SimpleLLM:
    """LLM simple si Ollama n'est pas disponible"""
    
    def __init__(self, model="simulation", temperature=0.2):
        self.model = model
        self.temperature = temperature
        print(f"âš ï¸ Mode simulation LLM activÃ©")
    
    def invoke(self, prompt: str) -> str:
        """Simulation de rÃ©ponse"""
        return f"""
ðŸ¤– **Mode Simulation**

Pour obtenir de vraies rÃ©ponses IA :
1. Installez Ollama: curl -fsSL https://ollama.ai/install.sh | sh
2. TÃ©lÃ©chargez le modÃ¨le: ollama pull deepseek-coder:6.7b
3. DÃ©marrez Ollama: ollama serve

En attendant, des templates basiques sont disponibles.
"""

class LLMHandler:
    """Gestionnaire des modÃ¨les de langage"""
    
    def __init__(self, model="deepseek-coder:6.7b", temperature=0.2):
        self.model = model
        self.temperature = temperature
        self.llm = self._setup_llm()
    
    def _setup_llm(self):
        """Configurer le LLM"""
        if not OLLAMA_AVAILABLE:
            return SimpleLLM()
        
        try:
            self._wait_for_ollama()
            llm = OllamaLLM(
                model=self.model,
                temperature=self.temperature
            )
            print("âœ… Ollama LLM configurÃ©")
            return llm
        except Exception as e:
            print(f"âš ï¸ Ollama non disponible: {e}")
            return SimpleLLM()
    
    def _wait_for_ollama(self, max_retries: int = 5):
        """Attendre qu'Ollama soit disponible"""
        for i in range(max_retries):
            try:
                response = requests.get("http://localhost:11434/api/version", timeout=2)
                if response.status_code == 200:
                    return
            except:
                pass
            if i == 0:
                print("â³ VÃ©rification Ollama...")
            time.sleep(1)
        raise Exception("Ollama non disponible")
    
    def invoke(self, prompt: str) -> str:
        """Invoquer le LLM"""
        try:
            response = self.llm.invoke(prompt)
            if hasattr(response, 'content'):
                return response.content
            return str(response)
        except Exception as e:
            print(f"âš ï¸ Erreur LLM: {e}")
            return f"Erreur lors de la gÃ©nÃ©ration: {e}"
    
    def is_available(self) -> bool:
        """VÃ©rifier si le LLM est disponible"""
        return OLLAMA_AVAILABLE and not isinstance(self.llm, SimpleLLM)